{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.layers.merge import concatenate as concat\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import losses, callbacks, metrics\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from scipy.misc import imsave\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import win32api\n",
    "import random\n",
    "from susyai13tev import susyai\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=pd.read_csv(\"data.csv\")\n",
    "data=raw_data.values\n",
    "index=data[:,0]\n",
    "truth_13tev=data[:,len(data[0,:])-1]\n",
    "'''remove first and last column'''\n",
    "data=np.delete(data,0,axis=1)\n",
    "data=np.delete(data,len(data[0,:])-1,axis=1)\n",
    "\n",
    "raw_data_label_extended=pd.read_csv(\"data_label_extended.csv\")\n",
    "data_label_extended=raw_data_label_extended.values\n",
    "data_label_extended=np.delete(data_label_extended,0,axis=1)\n",
    "truth_13tev = to_categorical(truth_13tev)\n",
    "data_label_extended=np.concatenate([data_label_extended, truth_13tev],axis=1)\n",
    "\n",
    "'''normalize'''\n",
    "for i in range(len(data[0,:])):\n",
    "    maximum=np.max(data[:,i])\n",
    "    minimum=np.min(data[:,i])\n",
    "    data[:,i]=(data[:,i]-minimum)/(maximum-minimum)\n",
    "\n",
    "for i in range(len(data_label_extended[0,:])):\n",
    "    maximum=np.max(data_label_extended[:,i])\n",
    "    minimum=np.min(data_label_extended[:,i])\n",
    "    data_label_extended[:,i]=(data_label_extended[:,i]-minimum)/(maximum-minimum)\n",
    "'''plots histograms if needed''' \n",
    "#for i in range(len(data[0,:])):\n",
    "#    plt.figure(i)\n",
    "#    plt.hist(data[:,i],100)\n",
    "#    plt.title(list(raw_data)[i+1])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data,data_label_extended, train_size=200000, test_size=100000)\n",
    "train = np.concatenate([x_train, y_train],axis=1)\n",
    "test = np.concatenate([x_test, y_test],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs,min_dim,hidden_layer_size,beta):\n",
    "    global cvae\n",
    "    global history\n",
    "    global decoder\n",
    "    global encoder\n",
    "    m = 1000 # batch size\n",
    "    n_z = min_dim # latent space size\n",
    "    encoder_dim1 = hidden_layer_size # dim of encoder hidden layer\n",
    "    decoder_dim = hidden_layer_size # dim of decoder hidden layer\n",
    "    decoder_out_dim = 19 # dim of decoder output layer\n",
    "    activ = 'relu'\n",
    "    optim = Adam(lr=0.001)\n",
    "    n_x = x_train.shape[1]\n",
    "    n_y = y_train.shape[1]\n",
    "    n_epoch = epochs\n",
    "\n",
    "    X = Input(shape=(n_x,))\n",
    "    data_label = Input(shape=(n_y,))\n",
    "    inputs = concat([X, data_label])\n",
    "    \n",
    "    encoder_1=Dense(encoder_dim1, activation=activ)(inputs)\n",
    "    encoder_2=Dense(encoder_dim1, activation=activ)(encoder_1)\n",
    "    encoder_3 = Dense(encoder_dim1, activation=activ)(encoder_2)\n",
    "    mu = Dense(n_z, activation='linear')(encoder_3)\n",
    "    l_sigma = Dense(n_z, activation='linear')(encoder_3)\n",
    "\n",
    "    def sample_z(args):\n",
    "        mu, l_sigma = args\n",
    "        eps = K.random_normal(shape=(m, n_z), mean=0, stddev=1)\n",
    "        return mu + K.exp(l_sigma / 2) * eps\n",
    "\n",
    "    # Sampling latent space\n",
    "    z = Lambda(sample_z, output_shape = (n_z, ))([mu, l_sigma])\n",
    "\n",
    "    # merge latent space with label\n",
    "    zc = concat([z, data_label])\n",
    "\n",
    "    decoder_1 = Dense(decoder_dim, activation=activ)(zc)\n",
    "    decoder_2 = Dense(decoder_dim, activation=activ)(decoder_1)\n",
    "    decoder_3 = Dense(decoder_dim, activation=activ)(decoder_2)\n",
    "    decoder_out = Dense(decoder_out_dim, activation='sigmoid')\n",
    "    #h_p = decoder_hidden(zc)\n",
    "    outputs = decoder_out(decoder_3)\n",
    "\n",
    "    def vae_loss(y_true, y_pred):\n",
    "        recon = K.sum(losses.mean_squared_error(y_true, y_pred), axis=-1)\n",
    "        kl = 0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=-1)\n",
    "        return recon + beta*kl\n",
    "\n",
    "    def KL_loss(y_true, y_pred):\n",
    "        return(0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=1))\n",
    "\n",
    "    def recon_loss(y_true, y_pred):\n",
    "        return K.sum(losses.mean_squared_error(y_true, y_pred), axis=-1)\n",
    "\n",
    "    cvae = Model([X, data_label], outputs)\n",
    "    encoder = Model([X, data_label], mu)\n",
    "\n",
    "    d_in = Input(shape=(n_z+n_y,))\n",
    "    d_h_1 = Dense(decoder_dim, activation=activ)(d_in)\n",
    "    d_h_2 = Dense(decoder_dim, activation=activ)(d_h_1)\n",
    "    d_h_3 = Dense(decoder_dim, activation=activ)(d_h_2)\n",
    "    d_out = decoder_out(d_h_3)\n",
    "    decoder = Model(d_in, d_out)\n",
    "\n",
    "    cvae.compile(optimizer=optim, loss=vae_loss, metrics = [KL_loss, recon_loss])\n",
    "    \n",
    "    earlystop=callbacks.EarlyStopping(monitor='val_loss',min_delta=0.00001,patience=1000,verbose=0, mode='auto')\n",
    "    filepath='C:\\\\Users\\\\chrisje\\\\stage\\\\best_extended'+str(min_dim)+'.hdf5'\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', \n",
    "                                           verbose=0, save_best_only=True, save_weights_only=True)\n",
    "    history=cvae.fit([x_train,y_train],x_train,\n",
    "            epochs=n_epoch, verbose=2,\n",
    "            batch_size=m,\n",
    "            validation_data=([x_test,y_test],x_test),\n",
    "            callbacks=[checkpoint,earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(epochs,min_dim,hidden_layer_size,beta):\n",
    "    global number_of_epochs\n",
    "    loss_list = [s for s in history.history.keys() if 'recon_loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'recon_loss' in s and 'val' in s]\n",
    "    number_of_epochs=len(history.history[loss_list[0]])\n",
    "    epochs = range(1,number_of_epochs + 1)\n",
    "    \n",
    "    plt.figure()\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l])\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l])\n",
    "    plt.title('Total loss extended '\n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs' )\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale('log')\n",
    "    plt.savefig('Total loss extended ' \n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs')\n",
    "    \n",
    "    plt.figure()\n",
    "    loss_list = [s for s in history.history.keys() if 'KL_loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'KL_loss' in s and 'val' in s]\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    plt.figure()\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l])\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l])\n",
    "    plt.title('KL loss extended '\n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale('log')\n",
    "    plt.savefig('KL loss extended '\n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs')\n",
    "    \n",
    "    plt.figure()\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s and 'KL' not in s and 'recon' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s and 'KL' not in s and 'recon' not in s]\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    plt.figure()\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l])\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l])\n",
    "    plt.title('MSE loss extended '\n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale('log')\n",
    "    plt.savefig('MSE loss extended '\n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs')\n",
    "              \n",
    "    plt.figure()\n",
    "    mse=[]\n",
    "    errors=x_test[:100000]-cvae.predict([x_test[:100000],y_test[:100000]],batch_size=1000)\n",
    "    for i in range(len(x_test[0,:])):\n",
    "        plt.figure()\n",
    "        plt.scatter(x_test[:1000,i],cvae.predict([x_test[:1000],y_test[:1000]],batch_size=1000)[:,i],alpha=1.0)\n",
    "        plt.plot([0,0], [1,1], 'k-',color='w')\n",
    "        error=errors[:,i]\n",
    "        mse=np.append(mse,np.mean([x**2 for x in error])) \n",
    "        plt.xlabel(list(raw_data)[i+1]  + 'test values')\n",
    "        plt.ylabel('prediction')\n",
    "        plt.title('Predictions extended '\n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs'\n",
    "                + list(raw_data)[i+1] )\n",
    "        \n",
    "        plt.savefig('Predictions extended '\n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs'\n",
    "                + list(raw_data)[i+1] )\n",
    "   \n",
    "    plt.figure()\n",
    "    plt.bar(range(len(mse)),mse,data=list(raw_data))\n",
    "    plt.yscale('log')\n",
    "    plt.title('MSE per variable extended ' \n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs')\n",
    "    plt.xlabel('variables')\n",
    "    plt.ylabel('mse')\n",
    "    plt.xticks(np.arange(19), list(raw_data)[1:20], rotation=30)\n",
    "    plt.savefig('MSE per variable extended '\n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs')\n",
    "\n",
    "    latent_space_collection=pd.DataFrame(data=encoder.predict([x_test[:1000],y_test[:1000]],batch_size=1000))\n",
    "    sns.pairplot(latent_space_collection).savefig('Pairplot extended ' \n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_numvec(digit, z=None):\n",
    "    out1 = np.zeros((1,19))\n",
    "    digit_one_hot = to_categorical(digit,2)\n",
    "    if z is None:\n",
    "        return np.concatenate([out1,digit_one_hot])\n",
    "    else:\n",
    "        return np.concatenate([z,digit_one_hot])\n",
    "\n",
    "def generate_new(epochs,min_dim,hidden_layer_size,beta):\n",
    "    digit = 1\n",
    "    sample_numbers = 150000\n",
    "\n",
    "    decoded_series=[]\n",
    "    for i in range(0, sample_numbers):\n",
    "        z=[np.mean(y_train[0,:]),np.mean(y_train[1,:]),np.mean(y_train[2,:])]\n",
    "        for j in range(0,min_dim):        \n",
    "            z.append(np.random.randn())\n",
    "        vec = construct_numvec(digit,z)\n",
    "        decoded = decoder.predict(np.reshape(vec,(1,min_dim+5)))\n",
    "        decoded=np.reshape(decoded,(19,))\n",
    "        decoded_series.append(decoded)\n",
    "    decoded_series=np.asarray(decoded_series)\n",
    "\n",
    "\n",
    "    data2=raw_data.values\n",
    "    data2=np.delete(data2,0,axis=1)\n",
    "    data2=np.delete(data2,len(data2[0,:])-1,axis=1)\n",
    "\n",
    "    '''de-normalize'''\n",
    "    for i in range(len(data2[0,:])):\n",
    "        maximum=np.max(data2[:,i])\n",
    "        minimum=np.min(data2[:,i])\n",
    "        decoded_series[:,i]=np.multiply((maximum-minimum),decoded_series[:,i])+minimum\n",
    "        \n",
    "    np.save('decoded_series extended '\n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs ',\n",
    "           decoded_series)\n",
    "    for i in range(len(data[0,:])):\n",
    "        plt.figure(i)\n",
    "        plt.hist(decoded_series[:,i],200)\n",
    "        plt.xlabel(list(raw_data)[i+1]  +' value')\n",
    "        plt.ylabel('count')\n",
    "        plt.title(' generated data historgram extended '\n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs'\n",
    "                + list(raw_data)[i+1] )\n",
    "        \n",
    "        plt.savefig(' generated data historgram extended '\n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs'\n",
    "                + list(raw_data)[i+1] )\n",
    "        plt.show()\n",
    "\n",
    "    for j in range(len(decoded_series[0,:])):\n",
    "        for i in range(j+1,len(decoded_series[0,:])):\n",
    "            plt.figure()\n",
    "            plt.hist2d(decoded_series[:,i],decoded_series[:,j], (100, 100), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "            plt.title(list(raw_data)[i+1] + ' vs ' + list(raw_data)[j+1] +' generated valid points log density distribution')\n",
    "            plt.xlabel(list(raw_data)[i+1])\n",
    "            plt.ylabel(list(raw_data)[j+1])\n",
    "            plt.colorbar()\n",
    "            plt.savefig(list(raw_data)[i+1] + ' vs ' + list(raw_data)[j+1] +' generated valid points log density distribution')\n",
    "            plt.show()\n",
    "        \n",
    "    digit = 0\n",
    "    sample_numbers = 150000\n",
    "\n",
    "    decoded_series=[]\n",
    "    for i in range(0, sample_numbers):\n",
    "        z=[np.mean(y_train[0,:]),np.mean(y_train[1,:]),np.mean(y_train[2,:])]\n",
    "        for j in range(0,min_dim):        \n",
    "            z.append(np.random.randn())\n",
    "        vec = construct_numvec(digit,z)\n",
    "        decoded = decoder.predict(np.reshape(vec,(1,min_dim+5)))\n",
    "        decoded=np.reshape(decoded,(19,))\n",
    "        decoded_series.append(decoded)\n",
    "    decoded_series=np.asarray(decoded_series)\n",
    "\n",
    "\n",
    "    data2=raw_data.values\n",
    "    data2=np.delete(data2,0,axis=1)\n",
    "    data2=np.delete(data2,len(data2[0,:])-1,axis=1)\n",
    "\n",
    "    '''de-normalize'''\n",
    "    for i in range(len(data2[0,:])):\n",
    "        maximum=np.max(data2[:,i])\n",
    "        minimum=np.min(data2[:,i])\n",
    "        decoded_series[:,i]=np.multiply((maximum-minimum),decoded_series[:,i])+minimum\n",
    "        \n",
    "    np.save('decoded_series_false extended '\n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs',\n",
    "           decoded_series)\n",
    "    for i in range(len(data[0,:])):\n",
    "        plt.figure(i)\n",
    "        plt.hist(decoded_series[:,i],200)\n",
    "        plt.xlabel(list(raw_data)[i+1]  +' value')\n",
    "        plt.ylabel('count')\n",
    "        plt.title(' generated data historgram false extended '\n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs'\n",
    "                + list(raw_data)[i+1] )\n",
    "        \n",
    "        plt.savefig(' generated data historgram false extended ' \n",
    "                + str(min_dim) + ' dim latent ' \n",
    "                + str(hidden_layer_size) + ' neuron hidden ' \n",
    "                + str('%.0E' % beta) + ' beta ' \n",
    "                + str(number_of_epochs) + ' epochs'\n",
    "                + list(raw_data)[i+1])\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    for j in range(len(decoded_series[0,:])):\n",
    "        for i in range(j+1,len(decoded_series[0,:])):\n",
    "            plt.figure()\n",
    "            plt.hist2d(decoded_series[:,i],decoded_series[:,j], (100, 100), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "            plt.title(list(raw_data)[i+1] + ' vs ' + list(raw_data)[j+1] +' generated non valid points log density distribution')\n",
    "            plt.xlabel(list(raw_data)[i+1])\n",
    "            plt.ylabel(list(raw_data)[j+1])\n",
    "            plt.colorbar()\n",
    "            plt.savefig(list(raw_data)[i+1] + ' vs ' + list(raw_data)[j+1] +' generated non valid points log density distribution')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all(epochs, min_dim, hidden_layer_size, beta):\n",
    "    print(epochs, min_dim, hidden_layer_size, beta)\n",
    "    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    train(epochs, min_dim, hidden_layer_size, beta)\n",
    "    plot(epochs, min_dim, hidden_layer_size, beta)\n",
    "    generate_new(epochs, min_dim, hidden_layer_size, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_list=[1000]\n",
    "min_dim_list=[17]\n",
    "hidden_layer_size_list=[54]\n",
    "beta_list=[0.01]\n",
    "for a in epoch_list:\n",
    "    for b in min_dim_list:\n",
    "        for c in hidden_layer_size_list:\n",
    "            for d in beta_list:\n",
    "                do_all(a,b,c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
